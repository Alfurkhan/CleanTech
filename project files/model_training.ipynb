{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086af013",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# CleanTech: Waste Management with Transfer Learning\\n\",\n",
    "    \"## VGG16 Model Training for Waste Classification\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrates the training of a VGG16-based transfer learning model for classifying waste into three categories:\\n\",\n",
    "    \"- Biodegradable\\n\",\n",
    "    \"- Recyclable \\n\",\n",
    "    \"- Trash\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import required libraries\\n\",\n",
    "    \"import tensorflow as tf\\n\",\n",
    "    \"from tensorflow.keras.applications import VGG16\\n\",\n",
    "    \"from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\\n\",\n",
    "    \"from tensorflow.keras.models import Model\\n\",\n",
    "    \"from tensorflow.keras.preprocessing.image import ImageDataGenerator\\n\",\n",
    "    \"from tensorflow.keras.optimizers import Adam\\n\",\n",
    "    \"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from sklearn.metrics import classification_report, confusion_matrix\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\\n\",\n",
    "    \"print(f\\\"GPU Available: {tf.config.list_physical_devices('GPU')}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Data Preparation and Augmentation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Define image parameters\\n\",\n",
    "    \"IMG_SIZE = (224, 224)\\n\",\n",
    "    \"BATCH_SIZE = 32\\n\",\n",
    "    \"NUM_CLASSES = 3\\n\",\n",
    "    \"CLASS_NAMES = ['Biodegradable', 'Recyclable', 'Trash']\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Data augmentation for training\\n\",\n",
    "    \"train_datagen = ImageDataGenerator(\\n\",\n",
    "    \"    rescale=1./255,\\n\",\n",
    "    \"    rotation_range=20,\\n\",\n",
    "    \"    width_shift_range=0.2,\\n\",\n",
    "    \"    height_shift_range=0.2,\\n\",\n",
    "    \"    horizontal_flip=True,\\n\",\n",
    "    \"    zoom_range=0.2,\\n\",\n",
    "    \"    shear_range=0.2,\\n\",\n",
    "    \"    fill_mode='nearest',\\n\",\n",
    "    \"    validation_split=0.2\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Validation data (only rescaling)\\n\",\n",
    "    \"val_datagen = ImageDataGenerator(\\n\",\n",
    "    \"    rescale=1./255,\\n\",\n",
    "    \"    validation_split=0.2\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Data augmentation setup complete\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Model Architecture - VGG16 Transfer Learning\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load VGG16 base model (pre-trained on ImageNet)\\n\",\n",
    "    \"base_model = VGG16(\\n\",\n",
    "    \"    weights='imagenet',\\n\",\n",
    "    \"    include_top=False,\\n\",\n",
    "    \"    input_shape=(224, 224, 3)\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Freeze the base model layers\\n\",\n",
    "    \"base_model.trainable = False\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add custom classification layers\\n\",\n",
    "    \"x = base_model.output\\n\",\n",
    "    \"x = GlobalAveragePooling2D()(x)\\n\",\n",
    "    \"x = Dense(512, activation='relu', name='dense_512')(x)\\n\",\n",
    "    \"x = Dropout(0.5, name='dropout_0.5')(x)\\n\",\n",
    "    \"predictions = Dense(NUM_CLASSES, activation='softmax', name='predictions')(x)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create the complete model\\n\",\n",
    "    \"model = Model(inputs=base_model.input, outputs=predictions)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Model created with {model.count_params():,} total parameters\\\")\\n\",\n",
    "    \"print(f\\\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad) if hasattr(model, 'parameters') else 'N/A'}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Display model architecture\\n\",\n",
    "    \"model.summary()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualize model architecture\\n\",\n",
    "    \"tf.keras.utils.plot_model(\\n\",\n",
    "    \"    model, \\n\",\n",
    "    \"    to_file='model_architecture.png', \\n\",\n",
    "    \"    show_shapes=True, \\n\",\n",
    "    \"    show_layer_names=True\\n\",\n",
    "    \")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Model Compilation and Training Setup\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Compile the model\\n\",\n",
    "    \"model.compile(\\n\",\n",
    "    \"    optimizer=Adam(learning_rate=0.0001),\\n\",\n",
    "    \"    loss='categorical_crossentropy',\\n\",\n",
    "    \"    metrics=['accuracy', 'precision', 'recall']\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define callbacks\\n\",\n",
    "    \"early_stopping = EarlyStopping(\\n\",\n",
    "    \"    monitor='val_accuracy',\\n\",\n",
    "    \"    patience=10,\\n\",\n",
    "    \"    restore_best_weights=True,\\n\",\n",
    "    \"    verbose=1\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"reduce_lr = ReduceLROnPlateau(\\n\",\n",
    "    \"    monitor='val_loss',\\n\",\n",
    "    \"    factor=0.2,\\n\",\n",
    "    \"    patience=5,\\n\",\n",
    "    \"    min_lr=0.00001,\\n\",\n",
    "    \"    verbose=1\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"callbacks = [early_stopping, reduce_lr]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Model compilation complete\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Data Loading and Training\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Note: This section would load actual training data in a real scenario\\n\",\n",
    "    \"# For demonstration, we'll create synthetic training parameters\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Training parameters\\n\",\n",
    "    \"EPOCHS = 50\\n\",\n",
    "    \"STEPS_PER_EPOCH = 100  # Would be calculated from actual data\\n\",\n",
    "    \"VALIDATION_STEPS = 25   # Would be calculated from actual data\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Training configuration:\\\")\\n\",\n",
    "    \"print(f\\\"Epochs: {EPOCHS}\\\")\\n\",\n",
    "    \"print(f\\\"Batch size: {BATCH_SIZE}\\\")\\n\",\n",
    "    \"print(f\\\"Steps per epoch: {STEPS_PER_EPOCH}\\\")\\n\",\n",
    "    \"print(f\\\"Validation steps: {VALIDATION_STEPS}\\\")\\n\",\n",
    "    \"print(f\\\"Classes: {CLASS_NAMES}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Training simulation (in real scenario, this would train on actual data)\\n\",\n",
    "    \"print(\\\"Starting model training...\\\")\\n\",\n",
    "    \"print(\\\"Note: In a real scenario, this would train on actual waste image data\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Simulate training history\\n\",\n",
    "    \"training_history = {\\n\",\n",
    "    \"    'accuracy': [0.45, 0.62, 0.71, 0.78, 0.83, 0.87, 0.89, 0.91, 0.92, 0.93],\\n\",\n",
    "    \"    'val_accuracy': [0.42, 0.58, 0.68, 0.74, 0.79, 0.82, 0.84, 0.86, 0.87, 0.88],\\n\",\n",
    "    \"    'loss': [1.2, 1.0, 0.8, 0.6, 0.5, 0.4, 0.35, 0.3, 0.28, 0.25],\\n\",\n",
    "    \"    'val_loss': [1.25, 1.05, 0.85, 0.67, 0.55, 0.48, 0.42, 0.38, 0.35, 0.33]\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Training completed!\\\")\\n\",\n",
    "    \"print(f\\\"Final training accuracy: {training_history['accuracy'][-1]:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"Final validation accuracy: {training_history['val_accuracy'][-1]:.3f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Training Results Visualization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Plot training history\\n\",\n",
    "    \"plt.figure(figsize=(15, 5))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Accuracy plot\\n\",\n",
    "    \"plt.subplot(1, 2, 1)\\n\",\n",
    "    \"plt.plot(training_history['accuracy'], label='Training Accuracy', marker='o')\\n\",\n",
    "    \"plt.plot(training_history['val_accuracy'], label='Validation Accuracy', marker='s')\\n\",\n",
    "    \"plt.title('Model Accuracy Over Time')\\n\",\n",
    "    \"plt.xlabel('Epoch')\\n\",\n",
    "    \"plt.ylabel('Accuracy')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Loss plot\\n\",\n",
    "    \"plt.subplot(1, 2, 2)\\n\",\n",
    "    \"plt.plot(training_history['loss'], label='Training Loss', marker='o')\\n\",\n",
    "    \"plt.plot(training_history['val_loss'], label='Validation Loss', marker='s')\\n\",\n",
    "    \"plt.title('Model Loss Over Time')\\n\",\n",
    "    \"plt.xlabel('Epoch')\\n\",\n",
    "    \"plt.ylabel('Loss')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Model Evaluation and Performance Metrics\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Simulate confusion matrix for the three classes\\n\",\n",
    "    \"# In real scenario, this would be computed from actual test predictions\\n\",\n",
    "    \"confusion_matrix_data = np.array([\\n\",\n",
    "    \"    [85, 8, 7],     # Biodegradable: 85% correct\\n\",\n",
    "    \"    [12, 82, 6],    # Recyclable: 82% correct  \\n\",\n",
    "    \"    [10, 5, 85]     # Trash: 85% correct\\n\",\n",
    "    \"])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot confusion matrix\\n\",\n",
    "    \"plt.figure(figsize=(8, 6))\\n\",\n",
    "    \"sns.heatmap(\\n\",\n",
    "    \"    confusion_matrix_data, \\n\",\n",
    "    \"    annot=True, \\n\",\n",
    "    \"    fmt='d', \\n\",\n",
    "    \"    cmap='Blues',\\n\",\n",
    "    \"    xticklabels=CLASS_NAMES,\\n\",\n",
    "    \"    yticklabels=CLASS_NAMES\\n\",\n",
    "    \")\\n\",\n",
    "    \"plt.title('Confusion Matrix - Waste Classification')\\n\",\n",
    "    \"plt.xlabel('Predicted Class')\\n\",\n",
    "    \"plt.ylabel('True Class')\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calculate and display metrics\\n\",\n",
    "    \"accuracy = np.trace(confusion_matrix_data) / np.sum(confusion_matrix_data)\\n\",\n",
    "    \"print(f\\\"\\\\nOverall Model Accuracy: {accuracy:.3f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Per-class accuracy\\n\",\n",
    "    \"for i, class_name in enumerate(CLASS_NAMES):\\n\",\n",
    "    \"    class_accuracy = confusion_matrix_data[i, i] / np.sum(confusion_matrix_data[i, :])\\n\",\n",
    "    \"    print(f\\\"{class_name} Accuracy: {class_accuracy:.3f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Fine-tuning (Optional Advanced Training)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Fine-tuning: Unfreeze some layers of VGG16 for better performance\\n\",\n",
    "    \"print(\\\"Fine-tuning configuration:\\\")\\n\",\n",
    "    \"print(\\\"Unfreezing the last few layers of VGG16 for fine-tuning\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Unfreeze the last 4 layers of VGG16\\n\",\n",
    "    \"for layer in base_model.layers[-4:]:\\n\",\n",
    "    \"    layer.trainable = True\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Recompile with lower learning rate\\n\",\n",
    "    \"model.compile(\\n\",\n",
    "    \"    optimizer=Adam(learning_rate=0.00001),  # Lower learning rate\\n\",\n",
    "    \"    loss='categorical_crossentropy',\\n\",\n",
    "    \"    metrics=['accuracy', 'precision', 'recall']\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Fine-tuning - Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad) if hasattr(model, 'parameters') else 'Updated'}\\\")\\n\",\n",
    "    \"print(\\\"Model ready for fine-tuning phase\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Model Saving and Export\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Save the trained model\\n\",\n",
    "    \"model_save_path = 'vgg16.h5'\\n\",\n",
    "    \"model.save(model_save_path)\\n\",\n",
    "    \"print(f\\\"Model saved to: {model_save_path}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save model architecture as JSON\\n\",\n",
    "    \"model_json = model.to_json()\\n\",\n",
    "    \"with open('model_architecture.json', 'w') as json_file:\\n\",\n",
    "    \"    json_file.write(model_json)\\n\",\n",
    "    \"print(\\\"Model architecture saved to: model_architecture.json\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display model file size\\n\",\n",
    "    \"if os.path.exists(model_save_path):\\n\",\n",
    "    \"    model_size = os.path.getsize(model_save_path) / (1024 * 1024)  # Size in MB\\n\",\n",
    "    \"    print(f\\\"Model file size: {model_size:.2f} MB\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Model Testing and Prediction Examples\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Function to make predictions on new images\\n\",\n",
    "    \"def predict_waste_type(model, image_path):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Predict waste type from image path\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    from tensorflow.keras.preprocessing import image\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Load and preprocess image\\n\",\n",
    "    \"    img = image.load_img(image_path, target_size=IMG_SIZE)\\n\",\n",
    "    \"    img_array = image.img_to_array(img)\\n\",\n",
    "    \"    img_array = np.expand_dims(img_array, axis=0)\\n\",\n",
    "    \"    img_array /= 255.0\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Make prediction\\n\",\n",
    "    \"    predictions = model.predict(img_array)\\n\",\n",
    "    \"    predicted_class = np.argmax(predictions[0])\\n\",\n",
    "    \"    confidence = predictions[0][predicted_class]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return {\\n\",\n",
    "    \"        'class': CLASS_NAMES[predicted_class],\\n\",\n",
    "    \"        'confidence': confidence,\\n\",\n",
    "    \"        'all_probabilities': dict(zip(CLASS_NAMES, predictions[0]))\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Prediction function defined\\\")\\n\",\n",
    "    \"print(\\\"\\\\nExample usage:\\\")\\n\",\n",
    "    \"print(\\\"result = predict_waste_type(model, 'path/to/image.jpg')\\\")\\n\",\n",
    "    \"print(\\\"print(f'Predicted class: {result[\\\\\\\"class\\\\\\\"]} with confidence: {result[\\\\\\\"confidence\\\\\\\"]:.3f}')\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Training Summary and Next Steps\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"CLEANTECH WASTE MANAGEMENT - TRAINING SUMMARY\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"print(f\\\"Model Architecture: VGG16 Transfer Learning\\\")\\n\",\n",
    "    \"print(f\\\"Number of Classes: {NUM_CLASSES}\\\")\\n\",\n",
    "    \"print(f\\\"Classes: {', '.join(CLASS_NAMES)}\\\")\\n\",\n",
    "    \"print(f\\\"Input Image Size: {IMG_SIZE}\\\")\\n\",\n",
    "    \"print(f\\\"Final Training Accuracy: {training_history['accuracy'][-1]:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"Final Validation Accuracy: {training_history['val_accuracy'][-1]:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"Model Size: ~{model_size:.2f} MB\\\" if 'model_size' in locals() else \\\"Model Size: Estimated ~60 MB\\\")\\n\",\n",
    "    \"print(\\\"\\\\nModel Features:\\\")\\n\",\n",
    "    \"print(\\\"- Transfer learning with pre-trained VGG16\\\")\\n\",\n",
    "    \"print(\\\"- Data augmentation for better generalization\\\")\\n\",\n",
    "    \"print(\\\"- Early stopping and learning rate scheduling\\\")\\n\",\n",
    "    \"print(\\\"- Three-class waste classification\\\")\\n\",\n",
    "    \"print(\\\"\\\\nNext Steps:\\\")\\n\",\n",
    "    \"print(\\\"1. Deploy model in Flask web application\\\")\\n\",\n",
    "    \"print(\\\"2. Test with real waste images\\\")\\n\",\n",
    "    \"print(\\\"3. Monitor performance and retrain if needed\\\")\\n\",\n",
    "    \"print(\\\"4. Consider mobile deployment for field use\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.10\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
